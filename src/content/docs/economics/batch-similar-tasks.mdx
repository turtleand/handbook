---
title: Batch Similar Tasks
description: Group related work into single API calls. Save tokens and respect rate limits.
---

# Batch Similar Tasks

Instead of making 10 separate AI calls for 10 similar tasks, make 1 call for all 10.

**Why:** Reduces token overhead, respects rate limits, and often produces more consistent results.

## The Problem with Serial Processing

**Scenario:** You need to summarize 10 articles.

**Naive approach:**
```
For each article:
  - Send article to AI
  - Get summary
  - Move to next article
```

**Costs:**
- 10 API calls (rate limit impact)
- 10× context setup overhead (system prompt, instructions repeated)
- 10× potential latency
- Inconsistent summaries (each request is independent)

## The Batching Alternative

**Better approach:**
```
Batch all 10 articles into one request:
  - Send all 10 articles
  - Get all 10 summaries in one response
```

**Benefits:**
- 1 API call (90% fewer calls)
- 1× context setup (shared instructions)
- Consistent format across summaries
- Faster overall (one round trip)

## Real Examples

### Email Triage

**Don't:**
```
For each unread email:
  Ask AI: "Is this urgent?"
```

**Do:**
```
Send all unread emails in one batch:
  "Here are 15 emails. For each, determine:
   1. Urgency (high/medium/low)
   2. Category (work/personal/spam)
   3. Suggested action
   Return as structured JSON."
```

**Savings:** 15 API calls → 1 API call

### Content Review

**Don't:**
```
For each draft:
  Ask AI: "Review this for grammar and clarity"
```

**Do:**
```
Send all drafts:
  "Review these 5 drafts. For each:
   - Grammar issues
   - Clarity score (1-10)
   - Top 3 improvements
   Return structured results."
```

**Savings:** 5 API calls → 1 API call  
**Bonus:** Consistent evaluation criteria across all drafts

### Data Classification

**Don't:**
```
For each customer message:
  Ask AI: "Classify this as: question, complaint, or praise"
```

**Do:**
```
Send 50 messages:
  "Classify each as question/complaint/praise.
   Return as JSON array."
```

**Savings:** 50 API calls → 1 API call  
**Bonus:** Way under rate limits (many APIs cap at 50-100 calls/minute)

## The Heartbeat Pattern

Instead of creating separate cron jobs for each periodic check, batch them:

**Bad pattern:**
```
Cron A: Every 30 min, check email
Cron B: Every 30 min, check calendar
Cron C: Every 30 min, check mentions
```

**Result:** 3 API calls every 30 minutes

**Better pattern:**
```
Heartbeat: Every 30 min, check:
  - Email (any urgent unread?)
  - Calendar (events in next 24h?)
  - Mentions (anything needing response?)
  
Return consolidated update.
```

**Result:** 1 API call every 30 minutes  
**Savings:** 66% fewer calls

## When Batching Makes Sense

**Good candidates for batching:**
- Tasks with identical instructions
- Tasks that benefit from consistency
- High-volume repetitive work
- Periodic checks
- Classification/filtering

**Poor candidates for batching:**
- Tasks requiring different context per item
- Tasks where one failure should stop the batch
- Interactive workflows (user needs incremental feedback)
- Tasks exceeding context window when combined

## Implementation Patterns

### JSON Array Output

```
"Process these 10 items. Return as JSON array:
[
  { id: 1, result: '...', status: 'success' },
  { id: 2, result: '...', status: 'success' },
  ...
]"
```

**Benefit:** Structured, easy to parse, matches input order.

### Markdown Sections

```
"Summarize these 5 articles. Use this format:

## Article 1: [Title]
Summary: ...
Key points: ...

## Article 2: [Title]
Summary: ...
Key points: ...
"
```

**Benefit:** Human-readable, easy to extract sections.

### Delimited Text

```
"Translate these 20 phrases. Separate each with '---':

Original 1
Translation 1
---
Original 2
Translation 2
---
"
```

**Benefit:** Simple parsing, works for text-heavy output.

## The Context Window Limit

**Trade-off:** Batching saves API calls but consumes more tokens per call.

**Example:**
- 10 separate calls: 10× 500 tokens = 5,000 tokens
- 1 batched call: 1× 4,000 tokens = 4,000 tokens

**Net savings:** 20% fewer tokens + 90% fewer API calls

**But watch out:** If batch size exceeds context window (e.g., 200K tokens), you have to split anyway.

**Rule of thumb:** Batch as much as fits comfortably (leave room for response). If you hit limits, split into smaller batches.

## Rate Limit Math

Most AI APIs have rate limits like:
- 50 requests per minute
- 500,000 tokens per minute

**Unbatched scenario:**
- Process 500 items (1 call each)
- Hit 50 req/min limit
- Takes 10 minutes

**Batched scenario:**
- Process 500 items (10 batches of 50)
- 10 calls total
- Takes less than 1 minute

**10x faster** just by batching.

## The Mental Shift

When you encounter repetitive work, ask:

**Old thinking:** "I need to run AI on each of these."  
**New thinking:** "Can I run AI once on all of these?"

**Often, the answer is yes.**

---

**See also:**
- [Computation Isn't the Bottleneck](/economics/computation-not-bottleneck)
- [The 14.3% Daily Budget](/economics/daily-budget)
- [Sub-Agents for Parallelism](/workflow/sub-agents-parallelism)
